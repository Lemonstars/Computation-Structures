# Basic of Information

### What is "Information"?
* Data communicated or received that resolves uncertainty about a particular  
fact or circumstance.  

### Quantifying Information
Given discrete random variable X  
* N possible values:        x1, x2, x3, ..., xn  
* Associated probabilities: p1, p2, p3, ..., pn  

Information received when learning that choice was xi:  
I(xi)=log<sub>2</sub>(<sup>1</sup>/<sub>pi</sub>)  

### Probability & Information Content
data | P<sub>data</sub> | log<sub>2</sub>(<sup>1</sup>/<sub>p<sub>data</sub></sub>)  
--------------------- | ---------------- | ----------------  
a heart               | 13/52 | 2bits  
not the Ace of spades | 51/52 | 0.028bits  
a face card (J, Q, K) | 12/52 | 2.115bits  
the "suicide king"    |  1/52 | 5.7bits  

The more uncertainly is resolved by the data,  the more information we have received.  


### Entropy
In information theory, the *entropy* H(X) is the average amount of information contained  
in each piece of data received about the value of X:  
H(X)=E(I(X))=the sum of p<sub>i</sub>*log<sub>2</sub>(<sup>1</sup>/<sub>p<sub>i</sub></sub>) 
for 1<=i<=N  

### Meaning of Entropy
There exits three circumstances when transmiting data:  
* less than H(X) bits: not enough information to resolve the uncertainty about the values  
* more than H(X) bits: not make the most effective use of our resources  
* the average exactly H(X): the perfect encoding but always a tough goal  

### Encodings
Encoding for each symbol A B C D | Encoding for "ABBA" | unambiguous
------------- | ----------- | ------------
00 01 10  11  | 00 01 01 00 | :+1:
01 1  000 001 | 01 1  1  01 | :+1:
0  1  10  11  | 0  1  1  0  | :x:

* Fixed-length encoding  
* variable-length encoding  
* ambiguous! "ABB" "ADA" "ABC" can be arrived  

### Encodings as Binary Trees
If you build a binary tree for a proposed encoding and find that  
* there are no symbols labeling interior nodes  
* exactly one sysmbol at each leaf 

The encoding is good to go.  

### Fixed-lenghth Encodings
* If the symbols we are trying to encode occur with equal probability (or if we have no  
a priori reason to believe otherwise), the we'll use a fixed-length encoding.  

* Note: 
  * all leaves in the encoding's binary tree are the same distance from the root  
  * the entropy formula is simply N*(<sup>1</sup>/<sub>N</sub>)*(log<sub>2</sub>(N))
  =log<sub>2</sub>(N)  
  

  
